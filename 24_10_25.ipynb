{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheemapatel/nlp--/blob/main/24_10_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzi6Y4auqImg",
        "outputId": "04903fb5-5029-4cf7-d476-704e3e5fcf09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Task 1: Topic Number Tuning (LDA) Results ðŸ“Š\n",
            "\n",
            "### LDA with K = 3 Topics\n",
            "Topic 1: war, conflict, scale, technology, military\n",
            "Topic 2: trade, market, new, reveals, deals\n",
            "Topic 3: instability, market, predict, worldwide, financial\n",
            "\n",
            "### LDA with K = 5 Topics\n",
            "Topic 1: technology, scale, military, large, danger\n",
            "Topic 2: trade, war, conflict, instability, market\n",
            "Topic 3: market, instability, predict, worldwide, financial\n",
            "Topic 4: war, conflict, instability, political, historical\n",
            "Topic 5: trade, reveals, new, economy, deals\n",
            "\n",
            "### LDA with K = 8 Topics\n",
            "Topic 1: trade, stability, policy, peace, global\n",
            "Topic 2: instability, war, conflict, market, trade\n",
            "Topic 3: instability, war, conflict, market, trade\n",
            "Topic 4: war, conflict, instability, political, historical\n",
            "Topic 5: reveals, new, analysis, economy, deals\n",
            "Topic 6: instability, war, conflict, market, trade\n",
            "Topic 7: technology, scale, military, large, danger\n",
            "Topic 8: market, worldwide, predict, reports, downturn\n",
            "\n",
            "--------------------------------------------------\n",
            "## Task 2: WordNet Hypernyms & Hyponyms ðŸŒ²\n",
            "Keyword: **war** (war.n.01)\n",
            "Hypernyms (Broader): {'action', 'military_action'}\n",
            "Hyponyms (Narrower): {'jihad', 'jehad', 'biological_attack', 'bioattack', 'information_warfare', 'civil_war', 'IW', 'international_jihad', 'hot_war', 'BW', 'biologic_attack', 'psychological_warfare', 'limited_war', 'war_of_nerves', 'chemical_operations', 'biological_warfare', 'chemical_warfare', 'world_war'}\n",
            "\n",
            "--------------------------------------------------\n",
            "## Task 3: Jaccard Similarity with Bigrams ðŸ”—\n",
            "Document 1 Unigrams: {'financial', 'analysis', 'market', 'downturn', 'reports.', 'predicted', 'major'}\n",
            "Document 2 Unigrams: {'market', 'downturn', 'economic', 'analysis.', 'predicted', 'major', 'global'}\n",
            "Unigram-based Jaccard Similarity: **0.4000**\n",
            "---\n",
            "Document 1 Bigrams: {('analysis', 'predicted'), ('predicted', 'major'), ('downturn', 'financial'), ('financial', 'reports.'), ('market', 'analysis'), ('major', 'downturn')}\n",
            "Document 2 Bigrams: {('major', 'economic'), ('predicted', 'analysis.'), ('downturn', 'global'), ('economic', 'downturn'), ('global', 'market'), ('market', 'predicted')}\n",
            "Bigram-based Jaccard Similarity: **0.0000**\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.util import ngrams\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "corpus = [\n",
        "    \"economic stability is key to global peace and trade policy\",\n",
        "    \"market analysis reveals new trade deals boost the economy\",\n",
        "    \"historical accounts of the war show political instability and conflict\",\n",
        "    \"military technology advances increase the danger of large scale conflict and war\",\n",
        "    \"financial reports predict market downturn and instability worldwide\",\n",
        "]\n",
        "documents = [\n",
        "    \"The market analysis predicted a major downturn in financial reports.\",\n",
        "    \"A major economic downturn in the global market was predicted by analysis.\"\n",
        "]\n",
        "doc1_text = documents[0].lower()\n",
        "doc2_text = documents[1].lower()\n",
        "\n",
        "stopwords = set(['a', 'the', 'in', 'of', 'and', 'by', 'was'])\n",
        "\n",
        "def tokenize_and_filter(text):\n",
        "    tokens = text.split()\n",
        "    return [token for token in tokens if token not in stopwords]\n",
        "\n",
        "\n",
        "def run_lda_and_display(corpus, n_topics_list):\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "    data_vectorized = vectorizer.fit_transform(corpus)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    lda_results = {}\n",
        "\n",
        "    for n_topics in n_topics_list:\n",
        "        lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=10)\n",
        "        lda.fit(data_vectorized)\n",
        "\n",
        "        top_words = []\n",
        "        for topic_idx, topic in enumerate(lda.components_):\n",
        "            top_features_ind = topic.argsort()[:-6:-1]\n",
        "            top_words.append([feature_names[i] for i in top_features_ind])\n",
        "\n",
        "        lda_results[n_topics] = top_words\n",
        "\n",
        "    return lda_results\n",
        "\n",
        "lda_output = run_lda_and_display(corpus, [3, 5, 8])\n",
        "\n",
        "print(\"## Task 1: Topic Number Tuning (LDA) Results ðŸ“Š\")\n",
        "for k, topics in lda_output.items():\n",
        "    print(f\"\\n### LDA with K = {k} Topics\")\n",
        "    for i, words in enumerate(topics):\n",
        "        print(f\"Topic {i+1}: {', '.join(words)}\")\n",
        "\n",
        "\n",
        "\n",
        "keyword = \"war\"\n",
        "synsets = wordnet.synsets(keyword, pos=wordnet.NOUN)\n",
        "target_synset = synsets[0]\n",
        "\n",
        "hypernyms = [lemma.name() for hyper in target_synset.hypernyms() for lemma in hyper.lemmas()]\n",
        "hyponyms = [lemma.name() for hypo in target_synset.hyponyms() for lemma in hypo.lemmas()]\n",
        "\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "print(\"## Task 2: WordNet Hypernyms & Hyponyms ðŸŒ²\")\n",
        "print(f\"Keyword: **{keyword}** ({target_synset.name()})\")\n",
        "print(f\"Hypernyms (Broader): {set(hypernyms)}\")\n",
        "print(f\"Hyponyms (Narrower): {set(hyponyms)}\")\n",
        "def calculate_jaccard_similarity(set1, set2):\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "    return len(intersection) / len(union) if len(union) > 0 else 0.0\n",
        "\n",
        "doc1_unigrams = set(tokenize_and_filter(doc1_text))\n",
        "doc2_unigrams = set(tokenize_and_filter(doc2_text))\n",
        "\n",
        "def get_bigram_set(text):\n",
        "    tokens = tokenize_and_filter(text)\n",
        "    return set(ngrams(tokens, 2))\n",
        "\n",
        "doc1_bigrams = get_bigram_set(doc1_text)\n",
        "doc2_bigrams = get_bigram_set(doc2_text)\n",
        "\n",
        "jaccard_unigram = calculate_jaccard_similarity(doc1_unigrams, doc2_unigrams)\n",
        "jaccard_bigram = calculate_jaccard_similarity(doc1_bigrams, doc2_bigrams)\n",
        "\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "print(\"## Task 3: Jaccard Similarity with Bigrams ðŸ”—\")\n",
        "print(f\"Document 1 Unigrams: {doc1_unigrams}\")\n",
        "print(f\"Document 2 Unigrams: {doc2_unigrams}\")\n",
        "print(f\"Unigram-based Jaccard Similarity: **{jaccard_unigram:.4f}**\")\n",
        "print(\"---\")\n",
        "print(f\"Document 1 Bigrams: {doc1_bigrams}\")\n",
        "print(f\"Document 2 Bigrams: {doc2_bigrams}\")\n",
        "print(f\"Bigram-based Jaccard Similarity: **{jaccard_bigram:.4f}**\")"
      ]
    }
  ]
}