{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheemapatel/nlp--/blob/main/8_8_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mo8nvmUkvZPE",
        "outputId": "75ed4aff-0cce-46b4-e293-34b9dd04ec8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Section A: Load & Explore\n",
            "Q1. First 3 rows of simulated resume data:\n",
            "\n",
            "| resume_text                                                                                                                                |\n",
            "|:-------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| Data Scientist. Core Skills: Python, Machine Learning, SQL, PyTorch. 5 years exp.                                                          |\n",
            "| Projects: Built a • predictive model using • scikit-learn and TensorFlow.                                                                  |\n",
            "| Senior Software Engineer. Languages: Java, C++, Python. Worked on REST APIs. 8+ yrs experience. Certifications: AWS, Kubernetes.           |\n",
            "| Marketing Specialist. Experienced in Digital Marketing, SEO, Google Analytics 4 (GA4). Handled $10k+ budgets. Strong communication skills. |\n",
            "\n",
            "---\n",
            "Initial inspection shows noisy characters like '\\n' (newline), '•' (bullet point), and '$', which need cleaning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "import spacy\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Simulated Resume Data\n",
        "data = {\n",
        "    'resume_text': [\n",
        "        \"Data Scientist. Core Skills: Python, Machine Learning, SQL, PyTorch. 5 years exp.\\nProjects: Built a • predictive model using • scikit-learn and TensorFlow.\",\n",
        "        \"Senior Software Engineer. Languages: Java, C++, Python. Worked on REST APIs. 8+ yrs experience. Certifications: AWS, Kubernetes.\",\n",
        "        \"Marketing Specialist. Experienced in Digital Marketing, SEO, Google Analytics 4 (GA4). Handled $10k+ budgets. Strong communication skills.\"\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Q1. Load sample resumes and display first 3 rows. Check for noisy characters.\n",
        "print(\"## Section A: Load & Explore\")\n",
        "print(\"Q1. First 3 rows of simulated resume data:\\n\")\n",
        "print(df.head(3).to_markdown(index=False))\n",
        "print(\"\\n---\")\n",
        "print(\"Initial inspection shows noisy characters like '\\\\n' (newline), '•' (bullet point), and '$', which need cleaning.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize NLTK tools\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def nltk_pipeline(text):\n",
        "    # 1. Clean special characters and digits\n",
        "    # Retain only letters and spaces, convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
        "\n",
        "    # 2. Tokenize\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # 3. Remove stop words and stem\n",
        "    processed_tokens = []\n",
        "    for word in tokens:\n",
        "        if word not in stop_words:\n",
        "            stemmed_word = stemmer.stem(word)\n",
        "            processed_tokens.append(stemmed_word)\n",
        "\n",
        "    return processed_tokens\n",
        "\n",
        "df['nltk_tokens'] = df['resume_text'].apply(nltk_pipeline)\n",
        "\n",
        "# Combine all tokens for frequency analysis\n",
        "all_nltk_tokens = [token for sublist in df['nltk_tokens'] for token in sublist]\n",
        "\n",
        "# Q2. Extract top 10 frequent stemmed words\n",
        "token_counts = Counter(all_nltk_tokens)\n",
        "top_10_nltk = token_counts.most_common(10)\n",
        "\n",
        "print(\"\\n## Section B: NLTK Preprocessing\")\n",
        "print(\"Q2. Top 10 Frequent Stemmed Words:\\n\")\n",
        "print(pd.DataFrame(top_10_nltk, columns=['Stemmed Word', 'Frequency']).to_markdown(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v1xghvcvcFs",
        "outputId": "33498b74-4e59-4576-a77a-9a66c8d29b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## Section B: NLTK Preprocessing\n",
            "Q2. Top 10 Frequent Stemmed Words:\n",
            "\n",
            "| Stemmed Word   |   Frequency |\n",
            "|:---------------|------------:|\n",
            "| skill          |           2 |\n",
            "| python         |           2 |\n",
            "| market         |           2 |\n",
            "| data           |           1 |\n",
            "| scientist      |           1 |\n",
            "| core           |           1 |\n",
            "| machin         |           1 |\n",
            "| learn          |           1 |\n",
            "| sql            |           1 |\n",
            "| pytorch        |           1 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "    spacy.cli.download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def spacy_pipeline(text):\n",
        "    # Process text using spaCy's pipeline (tokenization, POS, lemmatization)\n",
        "    doc = nlp(text)\n",
        "\n",
        "    filtered_lemmas = []\n",
        "\n",
        "    for token in doc:\n",
        "        # 1. Filter: Must be alphabetic (removes symbols, digits) and not a stop word\n",
        "        if token.is_alpha and not token.is_stop:\n",
        "\n",
        "            # 2. Filter: Only include Nouns and Verbs (common indicators of skills/actions)\n",
        "            if token.pos_ in (\"NOUN\", \"VERB\"):\n",
        "                # 3. Lemmatize and lowercase\n",
        "                lemma = token.lemma_.lower()\n",
        "                filtered_lemmas.append(lemma)\n",
        "\n",
        "    return filtered_lemmas\n",
        "\n",
        "df['spacy_lemmas'] = df['resume_text'].apply(spacy_pipeline)\n",
        "\n",
        "# Combine all lemmas for frequency analysis\n",
        "all_spacy_lemmas = [lemma for sublist in df['spacy_lemmas'] for lemma in sublist]\n",
        "\n",
        "# Q3. Extract top 10 frequent lemmas\n",
        "lemma_counts = Counter(all_spacy_lemmas)\n",
        "top_10_spacy = lemma_counts.most_common(10)\n",
        "\n",
        "print(\"\\n## Section C: spaCy Pipeline\")\n",
        "print(\"Q3. Top 10 Frequent Lemmas (Nouns/Verbs only):\\n\")\n",
        "print(pd.DataFrame(top_10_spacy, columns=['Lemma (Noun/Verb)', 'Frequency']).to_markdown(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IiQFsvIvqcA",
        "outputId": "b2a2e39d-33d3-4422-acb0-eb33c99cccc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## Section C: spaCy Pipeline\n",
            "Q3. Top 10 Frequent Lemmas (Nouns/Verbs only):\n",
            "\n",
            "| Lemma (Noun/Verb)   |   Frequency |\n",
            "|:--------------------|------------:|\n",
            "| experience          |           2 |\n",
            "| year                |           1 |\n",
            "| project             |           1 |\n",
            "| build               |           1 |\n",
            "| model               |           1 |\n",
            "| scikit              |           1 |\n",
            "| learn               |           1 |\n",
            "| language            |           1 |\n",
            "| work                |           1 |\n",
            "| rest                |           1 |\n"
          ]
        }
      ]
    }
  ]
}