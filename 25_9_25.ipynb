{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheemapatel/nlp--/blob/main/25_9_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Gcpa941nd8D",
        "outputId": "0238af37-d4db-4794-ad84-b234afe35c1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Deep Learning Model Training and Evaluation üß†\n",
            "\n",
            "--- Training **LSTM** Model ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n",
            "**LSTM Performance on Test Set:**\n",
            "Accuracy: 0.3333, F1-Score: 0.5000\n",
            "\n",
            "--- Training **CNN** Model ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ed9061965c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step\n",
            "**CNN Performance on Test Set:**\n",
            "Accuracy: 1.0000, F1-Score: 1.0000\n",
            "\n",
            "--- Training **Bi-LSTM** Model ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ed905fc9580> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step\n",
            "**Bi-LSTM Performance on Test Set:**\n",
            "Accuracy: 1.0000, F1-Score: 1.0000\n",
            "\n",
            "## Performance Comparison üìà\n",
            "\n",
            "### Deep Learning vs. Traditional ML Metrics ###\n",
            "| Model                    |   Accuracy |   F1-Score |\n",
            "|:-------------------------|-----------:|-----------:|\n",
            "| Logistic Regression (ML) |     0.7    |       0.65 |\n",
            "| LSTM                     |     0.3333 |       0.5  |\n",
            "| CNN                      |     1      |       1    |\n",
            "| Bi-LSTM                  |     1      |       1    |\n",
            "\n",
            "## Error Analysis for Best DL Model: **CNN** üßê\n",
            "\n",
            "### Misclassified Disaster Tweets (False Negatives) ###\n",
            "| Tweet   | True Label   | Predicted Label   |\n",
            "|---------|--------------|-------------------|\n",
            "\n",
            "### Misclassified Irrelevant Tweets (False Positives) ###\n",
            "| Tweet   | True Label   | Predicted Label   |\n",
            "|---------|--------------|-------------------|\n",
            "\n",
            "## Conclusion on Model Performance ‚úÖ\n",
            "\n",
            "Based on the F1-Score, the **CNN** model achieved the highest performance (1.0000).\n",
            "In real-world disaster classification with a large dataset, Deep Learning models (LSTM/CNN) using **pre-trained Word2Vec/GloVe embeddings** typically provide significant improvements over traditional ML models.\n",
            "They achieve this by:\n",
            "* **Capturing Context:** Recurrent layers (LSTM/Bi-LSTM) understand word order and context in a sequence.\n",
            "* **Semantic Understanding:** Pre-trained embeddings provide semantic meaning (e.g., 'quake' is close to 'earthquake'), which traditional TF-IDF often misses.\n",
            "However, on small, heavily-cleaned datasets (like the one simulated), the improvement may not be drastic, as the simple feature set might not benefit fully from the complexity of a neural network.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "# Deep Learning Imports\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "# --- 1. Dataset and Preprocessing Setup (Re-simulating from previous steps) ---\n",
        "\n",
        "# Re-simulate the cleaned and labeled data\n",
        "data_sim = {\n",
        "    'final_text': [\n",
        "        'forest fire near la pray affected', 'saw new batman movie disaster lol',\n",
        "        'massive flood warning issued new york city stay safe', 'phone soo slow today might well throw',\n",
        "        'breaking news 70 magnitude earthquake rock japan damage widespread', 'new album firee cant stop listening',\n",
        "        'devastating tornado hit oklahoma emergency response needed now', 'check cool website',\n",
        "        'emergency crew working hard rescue people collapsed building', 'loving weather today everyone',\n",
        "        'help missing people after explosion', 'bad day traffic jam is worst', 'shelter needed urgent',\n",
        "        'amazing sunset must see'\n",
        "    ],\n",
        "    'target': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
        "}\n",
        "df = pd.DataFrame(data_sim)\n",
        "\n",
        "# Global parameters for DL models\n",
        "MAX_WORDS = 1000  # Vocabulary size for the tokenizer\n",
        "MAX_LEN = 20      # Max sequence length (padding/truncation)\n",
        "EMBEDDING_DIM = 100 # Must match the size of pre-trained vectors (e.g., GloVe 100d)\n",
        "\n",
        "X = df['final_text']\n",
        "y = df.target.values\n",
        "\n",
        "# Split the data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 2. Tokenization and Padding\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "\n",
        "\n",
        "# --- 3. Pre-trained Word2Vec/GloVe Embedding Layer Preparation ---\n",
        "\n",
        "# Load Pre-trained Embeddings (SIMULATION ONLY)\n",
        "# In a real environment, you would load the entire GloVe/Word2Vec file here.\n",
        "# For example:\n",
        "# embeddings_index = {}\n",
        "# with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "#     for line in f:\n",
        "#         values = line.split()\n",
        "#         word = values[0]\n",
        "#         coefs = np.asarray(values[1:], dtype='float32')\n",
        "#         embeddings_index[word] = coefs\n",
        "\n",
        "# SIMULATE a simple embedding index and matrix\n",
        "embeddings_index = {\n",
        "    'fire': np.random.rand(EMBEDDING_DIM), 'flood': np.random.rand(EMBEDDING_DIM),\n",
        "    'earthquake': np.random.rand(EMBEDDING_DIM), 'safe': np.random.rand(EMBEDDING_DIM),\n",
        "    'lol': np.random.rand(EMBEDDING_DIM), 'movie': np.random.rand(EMBEDDING_DIM),\n",
        "    'new': np.random.rand(EMBEDDING_DIM), 'needed': np.random.rand(EMBEDDING_DIM),\n",
        "    'rescue': np.random.rand(EMBEDDING_DIM), 'explosion': np.random.rand(EMBEDDING_DIM),\n",
        "    'shelter': np.random.rand(EMBEDDING_DIM), 'bad': np.random.rand(EMBEDDING_DIM)\n",
        "}\n",
        "word_index = tokenizer.word_index\n",
        "VOCAB_SIZE = min(MAX_WORDS, len(word_index) + 1)\n",
        "\n",
        "# Create the embedding matrix\n",
        "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i < MAX_WORDS:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "\n",
        "# --- 4. Deep Learning Model Architectures ---\n",
        "\n",
        "def create_embedding_layer():\n",
        "    return Embedding(\n",
        "        VOCAB_SIZE,\n",
        "        EMBEDDING_DIM,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=MAX_LEN,\n",
        "        trainable=False  # Crucial: Keep pre-trained weights fixed\n",
        "    )\n",
        "\n",
        "def build_lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(create_embedding_layer())\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_cnn_model():\n",
        "    model = Sequential()\n",
        "    model.add(create_embedding_layer())\n",
        "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_bi_lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(create_embedding_layer())\n",
        "    model.add(Bidirectional(LSTM(64)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# --- 5. Training and Evaluation ---\n",
        "\n",
        "print(\"## Deep Learning Model Training and Evaluation üß†\")\n",
        "\n",
        "models = {\n",
        "    \"LSTM\": build_lstm_model(),\n",
        "    \"CNN\": build_cnn_model(),\n",
        "    \"Bi-LSTM\": build_bi_lstm_model()\n",
        "}\n",
        "\n",
        "dl_results = {}\n",
        "history_dict = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n--- Training **{name}** Model ---\")\n",
        "    # Training with a few epochs due to small simulated dataset\n",
        "    history = model.fit(\n",
        "        X_train_padded, y_train,\n",
        "        epochs=10,\n",
        "        batch_size=8,\n",
        "        validation_split=0.1,\n",
        "        verbose=0\n",
        "    )\n",
        "    history_dict[name] = history\n",
        "\n",
        "    # Evaluation\n",
        "    y_pred_proba = model.predict(X_test_padded)\n",
        "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "    dl_results[name] = {'Accuracy': accuracy, 'F1-Score': f1, 'Predictions': y_pred}\n",
        "\n",
        "    print(f\"**{name} Performance on Test Set:**\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}\")\n",
        "\n",
        "# --- 6. Comparison with Traditional ML ---\n",
        "\n",
        "# Assuming Logistic Regression (LR) from previous step achieved these hypothetical scores\n",
        "lr_accuracy = 0.70\n",
        "lr_f1 = 0.65\n",
        "\n",
        "print(\"\\n## Performance Comparison üìà\")\n",
        "\n",
        "comparison_data = [\n",
        "    {'Model': 'Logistic Regression (ML)', 'Accuracy': lr_accuracy, 'F1-Score': lr_f1}\n",
        "]\n",
        "for name, metrics in dl_results.items():\n",
        "    comparison_data.append({'Model': name, 'Accuracy': metrics['Accuracy'], 'F1-Score': metrics['F1-Score']})\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(\"\\n### Deep Learning vs. Traditional ML Metrics ###\")\n",
        "print(comparison_df.round(4).to_markdown(index=False))\n",
        "\n",
        "\n",
        "# --- 7. Error Analysis (Using the best DL model's predictions) ---\n",
        "\n",
        "# Find the best DL model based on F1-Score\n",
        "best_dl_model_name = max(dl_results, key=lambda k: dl_results[k]['F1-Score'])\n",
        "best_dl_predictions = dl_results[best_dl_model_name]['Predictions']\n",
        "\n",
        "print(f\"\\n## Error Analysis for Best DL Model: **{best_dl_model_name}** üßê\")\n",
        "\n",
        "# Create a DataFrame of the test set results for easy analysis\n",
        "test_df = pd.DataFrame({\n",
        "    'Tweet': X_test.reset_index(drop=True),\n",
        "    'True Label': y_test,\n",
        "    'Predicted Label': best_dl_predictions.flatten()\n",
        "})\n",
        "\n",
        "# Misclassified Positive Tweets (False Negatives: True=1, Predicted=0)\n",
        "fn_tweets = test_df[(test_df['True Label'] == 1) & (test_df['Predicted Label'] == 0)].head(5)\n",
        "print(\"\\n### Misclassified Disaster Tweets (False Negatives) ###\")\n",
        "print(fn_tweets[['Tweet', 'True Label', 'Predicted Label']].to_markdown(index=False))\n",
        "\n",
        "# Misclassified Negative Tweets (False Positives: True=0, Predicted=1)\n",
        "fp_tweets = test_df[(test_df['True Label'] == 0) & (test_df['Predicted Label'] == 1)].head(5)\n",
        "print(\"\\n### Misclassified Irrelevant Tweets (False Positives) ###\")\n",
        "print(fp_tweets[['Tweet', 'True Label', 'Predicted Label']].to_markdown(index=False))\n",
        "\n",
        "\n",
        "# --- 8. Conclusion ---\n",
        "\n",
        "print(\"\\n## Conclusion on Model Performance ‚úÖ\")\n",
        "best_overall_score = comparison_df['F1-Score'].max()\n",
        "best_overall_model = comparison_df.loc[comparison_df['F1-Score'].idxmax(), 'Model']\n",
        "\n",
        "print(f\"\\nBased on the F1-Score, the **{best_overall_model}** model achieved the highest performance ({best_overall_score:.4f}).\")\n",
        "\n",
        "if 'LSTM' in best_overall_model or 'CNN' in best_overall_model:\n",
        "    print(\"In real-world disaster classification with a large dataset, Deep Learning models (LSTM/CNN) using **pre-trained Word2Vec/GloVe embeddings** typically provide significant improvements over traditional ML models.\")\n",
        "    print(\"They achieve this by:\")\n",
        "    print(\"* **Capturing Context:** Recurrent layers (LSTM/Bi-LSTM) understand word order and context in a sequence.\")\n",
        "    print(\"* **Semantic Understanding:** Pre-trained embeddings provide semantic meaning (e.g., 'quake' is close to 'earthquake'), which traditional TF-IDF often misses.\")\n",
        "    print(\"However, on small, heavily-cleaned datasets (like the one simulated), the improvement may not be drastic, as the simple feature set might not benefit fully from the complexity of a neural network.\")\n",
        "else:\n",
        "    print(\"The high performance of the Traditional ML model (Logistic Regression) in this comparison suggests that for the small feature set provided, the linear separability achieved by TF-IDF is highly effective.\")"
      ]
    }
  ]
}